## 제1절 분산 데이터 저장 기술

 대규모 클러스터 시스템 플랫폼은 네트워크상에 분산된 서버들을 클러스터링함으로써 대용량 저장 공간과 빠른 처리 성능을 제공함. 분산 데이터 저장 기술은 네트워크상에서 데이터를 저장/조회/관리할 수 있으며, 저장 데이터를 정형화 여부와 데이터 모델에 따라 분산 파일시스템과 클러스터 데이터베이스, Key-Value 저장소 정도로 구분할 수 있음.

### 1. 분산 파일 시스템

대규모의 클러스터 시스템 플랫폼은 네트워크상에 분산된 많은 서버들을 클러스터로 구성함으로써 대용량의 저장 공간과 빠른 처리 성능을 제공할 수 있어야 함. 또한 시스템 확장이 쉽고, 서버 고장과 같은 시스템 장애가 발생하더라도 계속해서 안전하게 서비스를 제공할 수 있는 신뢰성과 가용성을 보장하여야 함. 비대칭형(asymmetric) 클러스터 파일 시스템은 성능과 확장성, 가용성 면에서 적합한 분산 파일 시스템 구조로, 최근에 연구와 개발이 활발히 진행되고 있음. 비대칭형 클러스터 파일 시스템에서는 파일 메타데이터를 관리하는 전용 서버를 별도로 둠으로써 메타데이터에 접근하는 경로와 데이터에 접근하는 경로를 분리해, 이를 통하여 파일 입출력 성능을 높이면서 독립적인 확장과 안전한 파일 서비스를 제공. 하지만 메타데이터 서버에 부하가 집중될 수 있으며 single-of-failure 지점이 될 수 있는 문제점도 내포하고 있음. 클러스터 파일 시스템은 구글에서 GFS(Google File System)에 대한 논문을 발표하면서 포털이나 검색 서비스 업체에서 많은 관심을 보이기 시작했으며, 최근에는 야후에서 적극적으로 지원하여 개발된 아파치의 하둡 파일 시스템(Haddop File System)까지 등장함.

#### 가. 구글 파일 시스템

구글 파일 시스템은 구글의 대규모 클러스터 서비스 플랫폼의 기반이 되는 파일 시스템으로 개발됨.

- 저가형 서버로 구성된 환경으로 서버의 고장이 빈번히 발생할 수 있다고 가정.
- 대부분의 파일은 대용량이라고 가정. 따라서 대용량 파일을 효과적으로 관리할 수 있는 방법이 요구됨.
- 작업 부하는 주로 연속적으로 많은 데이터를 읽는 연산이거나 임의의 영역에서 적은 데이터를 읽는 연산.
- 파일에 대한 쓰기 연산은 주로 순차적으로 데이터를 추가하며, 파일에 대한 갱신은 드물게 이루어짐.
- 여러 클라이언트에서 동시에 동일한 파일에 데이터를 추가하는 환경에서 동기화 오버헤드를 최소화할 수 있는 방법이 요구됨.
- 낮은 응답 지연시간보다 높은 처리율이 보다 중요함.

GFS에서 파일은 고정된 크기의 chunk들로 나누어 chunk 서버들에 분산.저장됨,. 그리고 각 chunk에 대한 여러 개의 복제본도 chunk 서버에 분산/저장됨. 따라서 클라이언트는 파일에 접근하기 위하여 마스터로부터 해당 파일의 chunk가 저장된 chunk 서버의 위치와 핸들을 먼저 받아옴.

이어서 직접 chunk 서버로 파일 데이터를 요청함. GFS의 마스터는 단일 마스터 구조로 파일 시스템 이름 공간과 파일의 chunk 매핑 정보, 각 chunk가 저장된 chunk 서버들의 위치 정보 들 모든 메타데이터를 메모리상에 관리함. GFS에서는 기본 chunk의 크기를 64MB로 지정함으로써 파일 메타데이터의 크기를 줄임. 또한 기존의 트리구조가 아닌 해시 테이블 구조 등을 활용함으로써 메모리상에서 보다 효율적인 메타데이터 처리를 지원함. 마스터에서는 주기적으로 하트비트(heartbeat) 메시지를 이용하여 chunk서버에 저장된 chunk들의 상태를 체크해 상태에 따라 chunk를 재복제하거나 재분산하는 것과 같은 회복 동작을 수행함.

마스터에 대한 장애 처리와 회복을 위해 파일시스템 이름 공간과 파일의 chunk 매핑 변경 연산을 로깅하고 마스터의 상태를 여러 섀도 마스터에 복제함.

Chunk 서버는 로컬 디스크에 chunk를 저장/관리하면서 클라이언트로부터의 chunk 입출력 요청을 처리함. chunk는 마스터에 의해 생성/삭제될 수 있으며, 유일한 식별자에 의해 구별됨. 마스터는 하나의 chunk 서버를 primary로 지정하여 복제본의 갱신 연산을 일관되게 처리할 수 있도록 보장함.

#### 나. 하둡 분산 파일 시스템

___[그림 II-2-2] HDFS 구조___

HDFS는 하나의 네임노드(NameNode)와 다수의 데이터노드(DataNode)로 구성됨. 네임노드는 파일 시스템의 이름 공간으 관리하면서 클라이언트로부터의 파일 접근 요청을 처리함. HDFS에서 파일 데이터는 블록 단위로 나뉘어 여러 데이터노드에 분산/저장됨. 그리고 블록들은 가용성을 보장하기 위하여 다시 복제/저장됨.

따라서 데이터노드는 클라이언트로부터 데이터 입출력 요청을 처리함. HDFS에서 파일은 한 번 쓰이면 변경되지 않는다고 가정함. 따라서 HDFS는 데이터에 대한 스트리밍 접근을 요청하며, 배치 작업에 적합한 응용을 대상으로 함.

네임노드는 데이터노드들로부터 하트비트(Heartbeat)를 주기적으로 받으면서 데이터노드들의 상태를 체크함. 또한 하트비트 메시지에 포함된 블록 정보를 가지고 블록의 상태를 체크할 수 있음.

HDFS는 클라이언트, 네임노드, 데이터노드 간의 통신을 위하여 TCP/IP 네트워크상에서 RPC(Remote Procedure Call)를 사용함.

#### 다. 러스터

___[그림 II-2-3] 러스터 구조___

러스터(Lustre) 클러스터 파일 시스템(Cluster File Systems Inc.)에서 개발한 객체 기반 클러스터 파일 시스템임. 러스터는 클라이언트 파일 시스템, 메타데이터 서버, 객체 저장 서버들로 구성되며, 이들은 고속 네트워크로 연결됨. 러스터에서는 계층화된 모듈 구조로 TCP/IP, 인피니밴드(Infiniband), 미리넷(Myrinet)과 같은 네트워크를 지원함. 클라이언트 파일 시스템은 리눅스 VFS(Virtual File System)에서 설치할 수 있는 파일 시스템으로, 메타데이터 서버와 객체 저장 서버들과 통신하면서 클라이언트 응용에 파일 시스템 인터페이스를 제공함. 메티데이터 서버는 파일 시스템의 이름 공간과 파일에 대한 메타데이터를 관리하며, 객체 저장 서버는 파일데이터를 저장하고 클라이언트로부터의 객체 입출력 요청을 처리함. 객체는 객체 저장 서버들에 스트라이핑되어 분산/저장됨.

러스터는 유닉스(Unix) 시맨틱을 제공하면서 파일 메타데이터에 대해서는 라이트백 캐시(Write Back Cache)를 지원함. 이를 위해 클라이언트에서 메타데이터 변경에 대한 갱신 레코드를 생성하고 나중에 메타데이터 서버를 전달함. 그러면 메타데이터 서버는 전달된 갱신 레코드를 재수행하여 변경된 메타데이터를 반영함. 더불어 메타데이터 서버에서는 메타데이터를 동시에 접근하는 부하에 따라 클라이언트 캐시에서 라이트백 캐시를 지원하거나 메타데이터 서버에서 메티데이터를 처리하는 방식을 적용함.

러스터는 메타데이터 서버에서 처리하도록 하는 방식을 사용해 메타데이터에 대한 동시 접근이 적으면 클라이언트 캐시를 이용한 라이트백 캐시를 사용하고, 메타데이터에 대한 동시 접근이 많으면 클라이언트 캐시를 사용함으로써 발생할 수 있는 오버헤드를 줄임.

러스터는 파일 메타데이터와 파일 데이터에 대한 동시성 제어를 위해 별도의 잠금을 사용함. 메타데이터에 접근하기 위해서는 메타데이터 서버의 잠금 서버로부터 잠금을 획득해야함. 파일 데이터를 접근하기 위해서는 해당 데이터가 저장된 객체 저장 서버의 잠금 서버로부터 잠금을 획득해야 함.

또한 러스터에서는 클라이언트와 메타데이터 서버 간의 네트워크 트래픽을 최소화하기 위하여 메타데이터에 대한 잠금 요청 시에 메타데이터 접근 의도를 같이 전달하는 인텐트(intent) 기반 잠금 프로토콜을 사용함. 따라서 메타데이터 서버는 메타데이터 접근 의도에 따라 해당 동작을 수행하고, 잠금을 승인하는 처리를 함께 수행함으로써 클라이언트와 메타데이터 서버 간의 네트워크 트래픽을 줄일 수 있음.

___[표 II-2-1] 클러스터 파일 시스템 비교___

### 2. 데이터베이스 클러스터

데이터를 통합할 때, 성능 향상과 가용성을 높이기 위해 데이터베이스 차원의 파티셔닝 또는 클러스터링을 이용함. 혜택은 다음과 같음.

- 파티션 사이의 병렬 처리를 통한 빠른 데이터 검색 및 처리 성능을 얻을 수 있음.
- 성능의 선형적인 증가 효과를 볼 수 있음.
- 특정 파티션에서 장애가 발생하더라도 서비스가 중단되지 않는 고가용성을 확보할 수 있음.

데이터베이스 시스템을 구성하는 형태에 따라 단일 서버  내의 파티셔닝과 다중 서버 사이의 파티셔닝으로 구분할 수 있음. 리소스 공유 관점에서는 다시 공유 디스크(Shared Disk)와 무공유(Shared Nothing)로 구분 할 수 있음.

#### 무공유

- 무공유 클러스터에서 각 데이터베이스 인스턴스는 자신이 관리하는 데이터 파일을 자신의 로컬 디스크에 저장하며, 이 파일들은 노드 간에 공유하지 않음.
- 각 인스턴스나 노드는 완전히 분리된 데이터의 서브 집합에 대한 소유권을 가지고 있으며, 각 데이터는 소유권을 갖고 있는 인스턴스가 처리함. 한 노드가 데이터 처리 요청을 받으면, 해당 노드는 처리할 데이터를 갖고 있는 노드에 신호를 보내 데이터 처리를 요청함. 무공유 구조의 장점은 노드 확장에 제한이 없는 것임. 단점은 각 노드에 장애가 발생할 경우를 대비해 별도의 폴트톨러런스(fault-tolerance)를 구성해야 한다는 것임.
- Oracle RAC(Real Application Cluster)를 제외한 대부분의 데이터베이스 클러스터가 무공유 방식을 채택하고 있음.

#### 공유 디스크

- 공유디스크 클러스터에서 데이터 파일은 논리적으로 모든 데이터베이스 인스턴스 노드들과 공유하며, 각 인스턴스는 모든 데이터에 접근할 수 있음. 데이터를 공유하려면 SAN(Storage Area Network)과 같은 공유 디스크가 반드시 있어야 하며, 모든 노드가 데이터를 수정할 수 있기 때문에 노드 간의 동기화 작업 수행을 위한 별도의 커뮤니케이션 채널이 필요함.
- 공유 디스크 방식의 가장 큰 장점은 높은 수준의 폴트톨러런스 제공임. 클러스터를 구성하는 노드 중 하나의 노드만 살아 있어도 서비스가 가능하기 때문. 단점은 클러스터가 커지면 디스크 영역에서 병목 현상이 발생함. Oracle RAC가 공유 디스크 방식을 이용하고 있음.

#### 가. Oracle RAC 데이터베이스 서버

Oracle RAC 데이터베이스 서버는 클러스터의 모든 노드에 실행되며, 데이터는 공유 스토리지에 저장됨. 클러스터의 모든 노드는 데이터베이스의 모든 테이블에 동등하게 엑세스하며, 특정 노드가 데이터를 '소유'하는 개념이 없음. 따라서 데이터를 파티셔닝할 필요가 없지만, 성능 향상을 위해 빈번하게 파티셔닝됨. 응용 프로그램은 클러스터의 특정 노느가 아니라 RAC 클러스터에 연결하며, RAC는 클러스터의 모든 노드에 로드를 고르게 분산함.

- 가용성
  - 클러스터의 한 노드가 어떤 이유로 장애를 일으키면 Oracle RAC는 나머지 노드에서 계속 실행됨. 장애가 발생한 노드에 연결됐던 모든 응용 프로그램은 투명하게 다시 연결되어 클러스터의 나머지 노드에 분산됨.
- 확장성
  - 추가 처리 성능이 필요하면 응용 프로그램이나 데이터베이스를 수정할 필요 없이 새 노드를 클러스터에 쉽게 추가할 수 있음. 클러스터의 모든 노드 간에 균형이 유지되도록 로드가 다시 분산됨. Oracle 10g R2 RAC는 클러스터 내에 최대 100개의 노드를 지원함.
- 비용 절감
  - RAC는 표준화된 소규모(CPU 4개 미만) 저가형 사용 하드웨어의 클러스터에서도 고가의 SMP 시스템만큼 효율적으로 응용 프로그램을 실행함으로써 비용을 절감함.

#### 나. IBM DB2 ICE(Integrated Cluster Environment)

DB2는 CPU/메모리/디스크를 파티션별로 독립적으로 운영하는 무공유 방식의 클러스터링을 지원함. 애플리케이션은 여러 파티션에 분산된 데이터베이스를 하나의 데이터베이스(Single View Database)로 보게되고, 데이터가 어느 파티션에 존재하고 있는지 알 필요가 없음. 따라서 데이터와 사용자가 증가하면 애플리케이션의 수정없이 기존 시스템에 노드를 추가하고 데이터를 재분배함으로써 시스템의 성능과 용량을 일정하게 유지할 수 있음.

하지만 각 노드로 분리되는 파티셔닝을 어떻게 구성하느냐에 따라 성능의 차이가 많이 발생할 수 있으며, 하나의 노드에 장애가 발생할 경우, 해당 노드에서 서비스하는 데이터에 대한 별도의 페일오버(failover) 메커니즘이 필요하게 됨. 따라서 DB2를 이용하여 클러스터를 구성할 때에도 가용성을 보장하기 위해 공유 디스크 방식을 이용함. 공유 디스크에 저장된 데이터 파일에 대해 특정 시점에서는 특정 노드에 의해 서비스 되지만 장애 상황이 발생하게 되면 다른 노드가 해당 데이터에 대한 서비스를 처리하는 방식으로 가용성을 보장함.

#### 다. 마이크로소프트 SQL Server

SQL Server는 연합(Federated) 데이터베이스 형태로 여러 노드로 확장할 수 있는 기능을 제공함. 연합 데이터베이스는 디스크 등을 공유하지 않는 독립된 서버에서 실행되는 서로 다른 데이터베이스들 간의 논리적인 결합이며, 네트워크를 이용하여 연결됨.

데이터는 관련된 서버들로 수평적으로 분할됨. 테이블을 논리적으로 분리해 물리적으로는 분산된 각 노드에 생성하고, 각 노드의 데이터베이스 인스턴스 사이에 링크를 구성한 후 모든 파티션에 대해 UNION ALL을 이용해 논리적인 뷰(VIEW)를 구성하는 방식으로 분산된 환경의 데이터에 대한 싱글 뷰를 제공함. SQL Server에서는 이런 뷰를 DVP(Distributed Partitioned View)라고 함.

이런 구성의 가장 큰 문제는 DBA나 개발자가 파티셔닝 정책에 맞게 테이블과 뷰를 생성해야 하고, 전역 스키마(Global schema)정보가 없기 때문에 질의 수행을 위해 모든 노드를 엑세스해야한다는 점이 있음. 노드의 개수가 작으면 간단하게 구성할 수 있지만, 노드가 많아지거나 노드의 추가/삭제가 발생하는 경우 파티션을 새로 해야 하는 문제도 있음. 또한 페일오버에 대해서는 별도로 구성해야 함. SQL Server에서도 페일오버에 대한 메커니즘을 제공하지만, Active-Active가 아닌 Active-Standby 방법을 상용하고 있음.

#### 라. MySQL

MySQL 클러스터는 무공유 구조에서 메모리 기반 데이터베이스의 클러스터링을 지원하며, 특정한 하트웨어 및 소프트웨어를 요구하지 않고 병렬 서버구조로 확장이 가능함.

MySQL 클러스터는 관리 노드(Management Node), 데이터 노드(NDB Node), MySQL 노드로 구성되며 다음과 같은 역할을 수행.

- 관리 노드 : 클러스터를 관리하는 노드로 클러스터 시작과 재구성 시에만 관여함
- 데이터 노드 : 클러스터의 데이터를 저장하는 노드
- MySQL 노드 : 클러스터 데이터에 접근을 지원하는 노드

MySQL 클러스터는 데이터의 가용성을 높이기 위해 데이터를 다른 노드에 복제시키며, 특정 노드에 장애가 발생하더라도 지속적인 데이터 서비스가 가능함. 장애가 났던 노드가 복구되어 클러스터에 투입된 경우에도 기존 데이터와 변경된 데이터에 대한 동기화 작업이 자동으로 수행됨. 데이터는 동기화 방식으로 복제되며, 이런 작업을 위해 일반적으로 데이터 노드 간에는 별도의 네트워크를 구성함.

MySQL의 최근 버전(5.1.6 이상)에서는 디스크 기반의 클러스터링을 제공함. 디스크 기반 클러스터링에서는 인덱스가 생성된 컬럼은 기존과 동일하게 메모리에 유지되지만, 인덱스를 생성하지 않은 컬럼은 디스크에 저장됨. 따라서 디스크에 저장된 데이터는 모두 인덱스가 없는 데이터임. 이 경우 디스크에 저장된 데이터와 JOIN 연산을 수행할 경우 성능이 좋지 않기 때문에 애플리케이션 개발시 주의 해야함. 디스크 기반이라 하더라도 인덱스로 구성된 컬럼은 메모리에 있ㄱ 때문에 데이터의 크기와 메모리 크기를 고려하여 인덱스 생성과 클러스터에 참여하는 장비의 메모리를 산정해야 함.